{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49e871a-4e00-455b-9c72-62a54d32f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weave\n",
      "  Downloading weave-0.52.17-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: click in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (8.3.1)\n",
      "Collecting diskcache==5.6.3 (from weave)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting eval-type-backport (from weave)\n",
      "  Downloading eval_type_backport-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting gql>=3.0.0 (from gql[aiohttp,requests]>=3.0.0->weave)\n",
      "  Downloading gql-4.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (4.25.0)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (25.0)\n",
      "Collecting polyfile-weave (from weave)\n",
      "  Downloading polyfile_weave-0.5.7-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (2.12.4)\n",
      "Collecting sentry-sdk<3.0.0,>=2.0.0 (from weave)\n",
      "  Downloading sentry_sdk-2.46.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (9.1.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from weave) (2025.2)\n",
      "Collecting wandb>=0.17.1 (from weave)\n",
      "  Downloading wandb-0.23.0-py3-none-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.11 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from sentry-sdk<3.0.0,>=2.0.0->weave) (2.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from sentry-sdk<3.0.0,>=2.0.0->weave) (2025.11.12)\n",
      "Collecting graphql-core<3.3,>=3.2 (from gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (1.22.0)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from anyio<5,>=3.0->gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from anyio<5,>=3.0->gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from anyio<5,>=3.0->gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (4.15.0)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yarl<2.0,>=1.6->gql>=3.0.0->gql[aiohttp,requests]>=3.0.0->weave) (0.4.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.11.2 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql[aiohttp,requests]>=3.0.0->weave) (3.13.2)\n",
      "Requirement already satisfied: requests<3,>=2.26 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql[aiohttp,requests]>=3.0.0->weave) (2.32.5)\n",
      "Requirement already satisfied: requests_toolbelt<2,>=1.0.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from gql[aiohttp,requests]>=3.0.0->weave) (1.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from aiohttp<4,>=3.11.2->gql[aiohttp,requests]>=3.0.0->weave) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from aiohttp<4,>=3.11.2->gql[aiohttp,requests]>=3.0.0->weave) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from aiohttp<4,>=3.11.2->gql[aiohttp,requests]>=3.0.0->weave) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from aiohttp<4,>=3.11.2->gql[aiohttp,requests]>=3.0.0->weave) (1.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests<3,>=2.26->gql[aiohttp,requests]>=3.0.0->weave) (3.4.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from jsonschema>=4.23.0->weave) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from jsonschema>=4.23.0->weave) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from jsonschema>=4.23.0->weave) (0.28.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pydantic>=2.0.0->weave) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pydantic>=2.0.0->weave) (0.4.2)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from wandb>=0.17.1->weave) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from wandb>=0.17.1->weave) (6.33.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from click->weave) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting abnf~=2.2.0 (from polyfile-weave->weave)\n",
      "  Downloading abnf-2.2.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting chardet>=5.0.0 (from polyfile-weave->weave)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting cint>=1.0.0 (from polyfile-weave->weave)\n",
      "  Downloading cint-1.0.0-py3-none-any.whl.metadata (511 bytes)\n",
      "Collecting fickling>=0.0.8 (from polyfile-weave->weave)\n",
      "  Downloading fickling-0.1.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting graphviz>=0.20.1 (from polyfile-weave->weave)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting intervaltree>=2.4.0 (from polyfile-weave->weave)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jinja2>=2.1.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from polyfile-weave->weave) (3.1.6)\n",
      "Collecting kaitaistruct~=0.10 (from polyfile-weave->weave)\n",
      "  Downloading kaitaistruct-0.11-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: networkx>=2.6.3 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from polyfile-weave->weave) (3.5)\n",
      "Collecting pdfminer.six<=20250506,>=20220524 (from polyfile-weave->weave)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=5.0.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from polyfile-weave->weave) (11.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from polyfile-weave->weave) (3.5.4)\n",
      "Collecting setuptools>=80.9.0 (from polyfile-weave->weave)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (46.0.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six<=20250506,>=20220524->polyfile-weave->weave) (2.23)\n",
      "Collecting stdlib-list~=0.11.1 (from fickling>=0.0.8->polyfile-weave->weave)\n",
      "  Downloading stdlib_list-0.11.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree>=2.4.0->polyfile-weave->weave)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chama\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from jinja2>=2.1.0->polyfile-weave->weave) (3.0.2)\n",
      "Downloading weave-0.52.17-py3-none-any.whl (765 kB)\n",
      "   ---------------------------------------- 0.0/765.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/765.9 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 524.3/765.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 765.9/765.9 kB 2.0 MB/s  0:00:00\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading sentry_sdk-2.46.0-py2.py3-none-any.whl (406 kB)\n",
      "Downloading gql-4.0.0-py3-none-any.whl (89 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading wandb-0.23.0-py3-none-win_amd64.whl (19.4 MB)\n",
      "   ---------------------------------------- 0.0/19.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/19.4 MB 4.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.6/19.4 MB 4.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 4.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.7/19.4 MB 4.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 5.0/19.4 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 6.3/19.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.8/19.4 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 7.6/19.4 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 8.4/19.4 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 8.7/19.4 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.9/19.4 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 10.0/19.4 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.7/19.4 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 11.5/19.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.5/19.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 11.8/19.4 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 12.3/19.4 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 12.8/19.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.2/19.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.2/19.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.4/19.4 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.5/19.4 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.5/19.4 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.5/19.4 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.5/19.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 17.0/19.4 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.6/19.4 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.8/19.4 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.4 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.4 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.4/19.4 MB 3.0 MB/s  0:00:06\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading eval_type_backport-0.3.0-py3-none-any.whl (6.1 kB)\n",
      "Downloading polyfile_weave-0.5.7-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.3/1.7 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.5 MB/s  0:00:00\n",
      "Downloading abnf-2.2.0-py3-none-any.whl (39 kB)\n",
      "Downloading kaitaistruct-0.11-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.1/5.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 3.5 MB/s  0:00:01\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading cint-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading fickling-0.1.5-py3-none-any.whl (46 kB)\n",
      "Downloading stdlib_list-0.11.1-py3-none-any.whl (83 kB)\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Building wheels for collected packages: intervaltree\n",
      "  Building wheel for intervaltree (setup.py): started\n",
      "  Building wheel for intervaltree (setup.py): finished with status 'done'\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26123 sha256=82d2c04a80eedd3169a6734198484db812cf369c85f739d700da91c6c6f58a47\n",
      "  Stored in directory: c:\\users\\chama\\appdata\\local\\pip\\cache\\wheels\\65\\c3\\c3\\238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
      "Successfully built intervaltree\n",
      "Installing collected packages: sortedcontainers, stdlib-list, smmap, setuptools, sentry-sdk, kaitaistruct, intervaltree, graphviz, graphql-core, eval-type-backport, diskcache, cint, chardet, abnf, gql, gitdb, fickling, pdfminer.six, gitpython, wandb, polyfile-weave, weave\n",
      "\n",
      "  Attempting uninstall: setuptools\n",
      "\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ----- ----------------------------------  3/22 [setuptools]\n",
      "   ------- --------------------------------  4/22 [sentry-sdk]\n",
      "   ------- --------------------------------  4/22 [sentry-sdk]\n",
      "   ------- --------------------------------  4/22 [sentry-sdk]\n",
      "   ------- --------------------------------  4/22 [sentry-sdk]\n",
      "   ------- --------------------------------  4/22 [sentry-sdk]\n",
      "   ------------ ---------------------------  7/22 [graphviz]\n",
      "   -------------- -------------------------  8/22 [graphql-core]\n",
      "   -------------- -------------------------  8/22 [graphql-core]\n",
      "   -------------- -------------------------  8/22 [graphql-core]\n",
      "   --------------------- ------------------ 12/22 [chardet]\n",
      "   --------------------- ------------------ 12/22 [chardet]\n",
      "   --------------------- ------------------ 12/22 [chardet]\n",
      "   --------------------- ------------------ 12/22 [chardet]\n",
      "   --------------------- ------------------ 12/22 [chardet]\n",
      "   ------------------------- -------------- 14/22 [gql]\n",
      "   ------------------------- -------------- 14/22 [gql]\n",
      "   ----------------------------- ---------- 16/22 [fickling]\n",
      "   ------------------------------ --------- 17/22 [pdfminer.six]\n",
      "   ------------------------------ --------- 17/22 [pdfminer.six]\n",
      "   -------------------------------- ------- 18/22 [gitpython]\n",
      "   -------------------------------- ------- 18/22 [gitpython]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ---------------------------------- ----- 19/22 [wandb]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   ------------------------------------ --- 20/22 [polyfile-weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   -------------------------------------- - 21/22 [weave]\n",
      "   ---------------------------------------- 22/22 [weave]\n",
      "\n",
      "Successfully installed abnf-2.2.0 chardet-5.2.0 cint-1.0.0 diskcache-5.6.3 eval-type-backport-0.3.0 fickling-0.1.5 gitdb-4.0.12 gitpython-3.1.45 gql-4.0.0 graphql-core-3.2.7 graphviz-0.21 intervaltree-3.1.0 kaitaistruct-0.11 pdfminer.six-20250506 polyfile-weave-0.5.7 sentry-sdk-2.46.0 setuptools-80.9.0 smmap-5.0.2 sortedcontainers-2.4.0 stdlib-list-0.11.1 wandb-0.23.0 weave-0.52.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth 2025.11.3 requires xformers>=0.0.27.post2; (\"linux\" in sys_platform or sys_platform == \"win32\") and (platform_machine == \"AMD64\" or platform_machine == \"x86_64\"), which is not installed.\n",
      "unsloth-zoo 2025.11.4 requires torchao>=0.13.0, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575e276a-2d74-4a69-a408-eec236e9026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"safelora_dpo_dataset.jsonl\",\n",
    "        \"eval\": \"safelora_eval_dpo_120.jsonl\",\n",
    "    },\n",
    ")\n",
    "\n",
    "train_dataset = data[\"train\"]\n",
    "eval_dataset = data[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bb9728-719b-468a-ba71-cdcced15cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1650 with Max-Q Design. Num GPUs = 1. Max memory: 4.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a26aca493045749445895a7cdd374a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87630197fbf41c19df15e6cf8920b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0560d9158f464e87ea897ed1825ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba5a44d828241ce8314a4f799b74cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=12):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f91cbd05be4e259aa80dd45ea241e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=12):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[0;32m     22\u001b[0m     model,\n\u001b[0;32m     23\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m training_args \u001b[38;5;241m=\u001b[39m DPOConfig(\n\u001b[0;32m     35\u001b[0m     per_device_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     36\u001b[0m     gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     padding_value \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id, \n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m dpo_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m dpo_trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\unsloth\\trainer.py:209\u001b[0m, in \u001b[0;36m_backwards_compatible_trainer.<locals>.new_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m trainer_kwargs\n\u001b[0;32m    208\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 209\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\safelora\\DPO\\unsloth_compiled_cache\\UnslothDPOTrainer.py:2692\u001b[0m, in \u001b[0;36mUnslothDPOTrainer.__init__\u001b[1;34m(self, model, ref_model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, **kwargs)\u001b[0m\n\u001b[0;32m   2690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2691\u001b[0m     model\u001b[38;5;241m.\u001b[39mfor_training()\n\u001b[1;32m-> 2692\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2695\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2706\u001b[0m     model\u001b[38;5;241m.\u001b[39mfor_inference()\n",
      "File \u001b[1;32mD:\\safelora\\DPO\\unsloth_compiled_cache\\UnslothDPOTrainer.py:907\u001b[0m, in \u001b[0;36m_UnslothDPOTrainer.__init__\u001b[1;34m(self, model, ref_model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_num_proc \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdataset_num_proc\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# Dataset preparation\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mD:\\safelora\\DPO\\unsloth_compiled_cache\\UnslothDPOTrainer.py:1097\u001b[0m, in \u001b[0;36m_UnslothDPOTrainer._prepare_dataset\u001b[1;34m(self, dataset, processing_class, args, dataset_name)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset):  \u001b[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying chat template to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1097\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaybe_apply_chat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset):  \u001b[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3323\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3320\u001b[0m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;241m=\u001b[39m prev_env\n\u001b[0;32m   3321\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3323\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[0;32m   3325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3328\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\datasets\\utils\\py_utils.py:619\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    617\u001b[0m             pool_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    618\u001b[0m             \u001b[38;5;66;03m# One of the subprocesses has died. We should not wait forever.\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    620\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the subprocesses has abruptly died during map operation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo debug the error, disable multiprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m             )\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing."
     ]
    }
   ],
   "source": [
    "import weave\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from unsloth import FastLanguageModel, PatchDPOTrainer, is_bfloat16_supported\n",
    "PatchDPOTrainer()\n",
    "\n",
    "import torch\n",
    "from trl import DPOTrainer, DPOConfig   \n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    device_map    = {\"\": 0}, \n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_ratio = 0.1,\n",
    "    num_train_epochs = 3,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    seed = 42,\n",
    "    output_dir = \"outputs\",\n",
    "    padding_value = tokenizer.pad_token_id, \n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = training_args,\n",
    "    beta = 0.1,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a56f98-aa34-46ad-9fd2-a2f75cd65d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
